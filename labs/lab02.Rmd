---
title: "Practice Assignment 2 MATH 343"
author: "Natasha Watson"
output: pdf_document
date: "noon April 11"
---

This practice assignment is coupled to the theory assignment (the problem numbers align herein) and should be worked on concomitantly. You will write code in places marked "TO-DO" to complete the problems. Most of this will be a pure programming assignment but there are some questions that instead ask you to "write a few sentences" which are not R chunks.

The tools for solving these problems can be found in the class demos located [here](https://github.com/kapelner/QC_MATH_343_Spring_2024/tree/main/demos).

To "hand in" the homework, push this completed file by the due date to your course repo.

NOT REQUIRED: After you're done, you have the option to compile this file into a PDF (use the "knit to PDF" button on the submenu above). These final PDF's look pretty as it includes the output of your code. You can push the PDF as well. It will look nice in your portfolio.

This lab requires the following packages. You should make sure they load before beginning:

```{r}
pacman::p_load(ggplot2, glmnet, survival, lmtest, skimr, MASS, mlbench, rstan)
```

## Problem 1: Inference for the linear model using the OLS estimator

Below is a design matrix taken from the boston housing data and a definition of some variables.

```{r}
X = model.matrix(medv ~ ., MASS::Boston)
n = nrow(X)
p_plus_one = ncol(X)
XtXinvXt = solve(t(X) %*% X) %*% t(X)
H = X %*% XtXinvXt
In_minus_H = diag(n) - H
```

We will now assume betas of all ones and a sigma of 2:

```{r}
betavec = rep(1, p_plus_one)
sigsq = 2^2
```

We will now simulate many response vectors using the core assumption. Remember that the `rnorm` function takes sigma (not sigma-squared) as an argument. Then we'll use the response vectors to compute b, yhat and e. We will collect them all into matrices so we can investigate their behavior later.

```{r}
Nsim = 10000
bs = matrix(NA, nrow = p_plus_one, ncol = Nsim)
yhats = matrix(NA, nrow = n, ncol = Nsim)
es = matrix(NA, nrow = n, ncol = Nsim)
set.seed(1)
for (nsim in 1 : Nsim){
  y = X %*% betavec + rnorm(n, mean = 0, sd = sqrt(sigsq))
  b = solve(t(X) %*% X) %*% t(X) %*% y
  yhat = X %*% b
  e = y - yhat
    bs[, nsim] = b
  yhats[, nsim] = yhat
  es[, nsim] = e
}
```

Let's now make sure the formulas are correct for Yhat. Let's take the 17th observation and standardize its values based on knowledge of the true betas and the formulas from class. We can plot them here:

```{r}
mu_17 = X[17, ] %*% betavec                     # x_17^T * beta
h_17 = H[17, 17]                                # leverage
var_17 = sigsq * h_17
yhat17s = yhats[17, ]
yhat17s_std = (yhat17s - as.numeric(mu_17)) / sqrt(var_17)
ggplot(data.frame(yhat17s_std = yhat17s_std)) + aes(x = yhat17s_std) + geom_histogram()
```

This distribution should look like a standard normal. Confirm that you cannot reject a Kolmogorov-Smirnov test that `yhat17s_std` comes from an iid N(0, 1) DGP:

Note - we do this KS test to statistically check whether the simulated quantities - after standardization match their theoretical distributions

```{r}
ks.test(yhat17s_std, "pnorm")
```
Our p-value was 0.89 - cannot reject null

Repeat this Kolmogorov-Smirnov test for the 7th entry of b.

```{r}
b7s = bs[7, ]
var_b7 = sigsq * solve(t(X) %*% X)[7, 7]
b7s_std = (b7s - 1) / sqrt(var_b7)

ks.test(b7s_std, "pnorm")
```

Repeat this Kolmogorov-Smirnov test for the 37th entry of e.

```{r}
e37s = es[37,]
var_e37 = sigsq * (1 - H[37,37]) #var of e37
e37s_std = e37s / sqrt(var_e37)

ks.test(e37s_std, "pnorm")
```

Now let's work with just one realization of the errors which gives us one estimate of y, b, yhat and e:

```{r}
b = bs[, 1] #vector of coefficients from 1st sim. dataset; p+1
yhat = yhats[, 1] #n length vector of fitted values for 1st sim
e = es[, 1] #u get the point
y = yhat + e
```

At level alpha = 5%, test H_0: beta_7 = 0 by calculating the t-statistic and comparing it to the appropriate critical value of t.

Note: sigsq is known  - CI = [b_j \pm 1.96 sig * sqrt(XtXinv)[j,j] ]
```{r}
b = bs[,1]
b7 = b[7]
e = es[, 1]
SSE = sum(e^2)
df_err = n - p_plus_one
MSE = SSE / df_err
MSE #MSE is an unbiased estimate of sigsq and thus should be approx sigsq = 4
s_e = sqrt(MSE)
s_e #RMSE is a consistent estimate of sigma and thus should be approx sigma = 2

#find se for b7
XtXinv = solve(t(X) %*% X)
se_b7 = s_e * sqrt(XtXinv[7,7])
t_stat = b7/se_b7

alpha = 0.05
t_one_minus_alpha_over_two_df = qt(1 - alpha / 2, df_err)
t_one_minus_alpha_over_two_df
t_stat
```
t_stat >> T_one_minus_alpha_over_two_df --> reject H_0 i.e. b7 matters 


Create a 95% CI for mu_17, the expected value of the 17th observation in the X matrix.

```{r}
b = bs[,1]
b7 = b[7]
e = es[, 1]
SSE = sum(e^2)
df_err = n - p_plus_one
MSE = SSE / df_err
MSE #MSE is an unbiased estimate of sigsq and thus should be approx sigsq = 4
s_e = sqrt(MSE)
s_e #RMSE is a consistent estimate of sigma and thus should be approx sigma = 2

x17 = X[17, , drop = FALSE]
mu_hat_17 = as.numeric(x17 %*% b)

XtXinv = solve(t(X) %*% X)
se_mu_17 = s_e * sqrt(x17 %*% XtXinv %*% t(x17))


t_crit = qt(0.975, df_err)
c(mu_hat_17 - t_crit * se_mu_17, mu_hat_17 + t_crit * se_mu_17)
```

Create a 95% CI for y_17, the response value for the 17th observation in the X matrix.

```{r}
b = bs[,1]
b7 = b[7]
e = es[, 1]
SSE = sum(e^2)
df_err = n - p_plus_one
MSE = SSE / df_err
s_e = sqrt(MSE) #rmse = estimate for sigma
XtXinv = solve(t(X) %*% X)

x17 = X[17, ,drop = FALSE]
yhat_17 = as.numeric(x17 %*% b)
se_pred_17 = sqrt(MSE *(1 + x17 %*% XtXinv %*% t(x17)))

alpha = 0.05
t_cr = qt(1 - alpha/2, df_err)
c(yhat_17 - t_cr*se_pred_17, yhat_17 + t_cr*se_pred_17 )
```

Run the omnibus test at level alpha = 5% by calculating the quantities from scratch and comparing to the appropriate critical F value.
 to test if full model is sig better than a model with just the intercept (knock everythin out except beta_0)
```{r}
b = bs[, 1]
yhat = yhats[, 1]
e = es[, 1]
y = yhat + e

p = p_plus_one - 1
SSR = sum((yhat - mean(y))^2)
MSR = SSR/p
SSE = sum(e^2)
MSE = SSE/(n-(p_plus_one))

F_stat = MSR/MSE
F_crit = qf(1 - 0.05, p, n - (p_plus_one))
F_stat > F_crit #if TRUE reject H_0, if FALSE retain
```
We rejected H_0 so our features DO matter

Run the multiple effect test for H_0: beta_1 = beta_2 = beta_3 = 0 at level alpha = 5% by calculating the quantities from scratch and comparing to the appropriate critical F value.

Recall the F-statistic for the partial test where S is the subset of features being knocked out and A are the features that you keep; |S| = k and |A| = p +1 - k
multiple effect test is aka partial F-test - you're testing a subset of features
```{r}
b_full = bs[, 1]
e_full = es[, 1]
SSE_full = sum(e_full^2)

#build reduced design matrix by dropping cols for beta_1,2,3 which are cols 2,3,4 (intercept = col 1)
X_red = X[, -c(2,3,4)]
b_red = solve(t(X_red) %*% X_red) %*% t(X_red) %*% (yhats[, 1] + es[,1]) #estimate coefficients for reduced model by fitting it to the same resposne
yhat_red = X_red %*% b_red #compute fitted values
e_red = (yhats[,1] + es[,1]) - yhat_red
SSE_red = sum(e_red^2)
k = 3

F_stat = ((SSE_red - SSE_full)/k)/(SSE_full/ n-p_plus_one)
F_crit = qf(1 - 0.05, k, n-p_plus_one)
F_stat > F_crit #if TRUE we rjct H_0 (at least on feature in S matters)
```

Compute the maximum likelihood estimator for sigsq.
- we showed in theory hw that MLE for sigsq is SSE/n
```{r}
print(SSE/n)
```

## Problem 2: Ridge and Lasso predictions

We'll use the data setup from class: the boston housing data with another 1000 garbage features tacked on and then all features standardized:

```{r}
rm(list = ls())
p_extra = 1000

set.seed(1)
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
X = cbind(X, matrix(rnorm(nrow(X) * p_extra), ncol = p_extra)) #appends 1000 random noise cols to the matrix which have no relationship with the response
colnames(X) = c("(Intercept)", colnames(MASS::Boston)[1:13], paste0("junk_", 1 : p_extra)) #labels cols of junk features after the oriiginal features as junk_1, junk_2, ..., junk_1000

#now we standardize the columns
X = apply(X, 2, function(x_dot_j){(x_dot_j - mean(x_dot_j)) / sd(x_dot_j)}) #essential for ridge and lasso regression as they are scale senstive bc/ they penalize coefficianet size
X[, 1] = 1 #reset the intercept
```

We will now split the data into training (with 400 observations) and test (106):

```{r}
train_idx = sample(1 : nrow(X), 400)
test_idx = setdiff(1 : nrow(X), train_idx) 
Xtrain = X[train_idx, ] #predictor matrix - 400 by 1014
ytrain = y[train_idx]
Xtest =  X[test_idx, ] #test set w remaining 106 observations
ytest =  y[test_idx]
```

In class we fit many ridge models and many lasso models using arbitrary values of lambda. Here we will use the model selection technique from 342W implementing inner K-fold CV but not the outer K-fold CV. We can use the `cv.glmnet` function to do this. You can use its default lambda grid search. Run both ridge and lasso. Report the optimal lambda values for ridge and lasso.

```{r}
library(glmnet)
#ridge 
set.seed(1)
cv_ridge = cv.glmnet(Xtrain, ytrain, alpha = 0, standardize = FALSE) #alpha = 0 shirnks all coefficients - l2 penalty
lambda_ridge_optimal = cv_ridge$lambda.min

# Lasso
set.seed(1)
cv_lasso = cv.glmnet(Xtrain, ytrain, alpha = 1, standardize = FALSE) #alpha = 1; shrinks and does variable selection so some coefficients become zero
lambda_lasso_optimal = cv_lasso$lambda.min

lambda_ridge_optimal
lambda_lasso_optimal
```

Now fit both the ridge and lasso models using their respective optimal values of lambda.

```{r}
ridge_mod = glmnet(Xtrain, ytrain, alpha = 0, lambda = lambda_ridge_optimal, standardize = FALSE)

lasso_mod = lasso_mod = glmnet(Xtrain, ytrain, alpha = 1, lambda = lambda_lasso_optimal, standardize = FALSE)
```

For the lasso model, which features did it select?

```{r}
lasso_coefs = coef(lasso_mod) #returns matrix where rows represent the features 

# non-zero coefficients (excluding the intercept)
selected_features = rownames(lasso_coefs)[lasso_coefs[, 1] != 0]
selected_features = selected_features[selected_features != "(Intercept)"]  # remove intercept

selected_features
```

Now predict on the test set and calculate oosRMSE. Who wins?

```{r}
pred_ridge = predict(ridge_mod, newx = Xtest)
pred_lasso = predict(lasso_mod, newx = Xtest)

oosRMSE_ridge = sqrt(mean((pred_ridge - ytest)^2))
oosRMSE_lasso = sqrt(mean((pred_lasso - ytest)^2))

oosRMSE_ridge
oosRMSE_lasso
```

Lasso won because it has a lower OOS RMSE
Ridge usually performs better when all predictors have small effect nd there is multicolinearity - lasso usually performs better when only a few predictors REALLY matter and you are attempting to identify important features.

# Problem 3: Robust regression methods

Let's use 1000 rows of the diamonds dataset for this exercise. We'll convert the ordinal factors to nominal to make the feature dummy names more readable.

```{r}
rm(list = ls())
diamonds = ggplot2::diamonds
?diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)      #convert to nominal
diamonds$color =    factor(diamonds$color, ordered = FALSE)    #convert to nominal
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)  #convert to nominal

set.seed(1)
idx = sample(1 : nrow(diamonds), 1000)
X = model.matrix(price ~ ., diamonds[idx, ])
y = diamonds$price[idx]
rm(list = setdiff(ls(), c("X", "y")))
```

Fit a linear model on all features and report the p-value for the test of H_0: beta_j = 0 where j is the index of the `depth` feature.

```{r}
lm_fit = lm(y ~ X - 1) #telling to not add an intcpt bc we alreadt have one
lm_summary = summary(lm_fit)
pvals = coef(lm_summary)[, "Pr(>|t|)"]
pval_depth = pvals["Xdepth"]
pval_depth
```
 
We saw pcal is 1% which is less than 5% therefore, reject H_0 - depth matters 

Now assume nothing is known about the error DGP except that they are independent.

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 where j is the index of the `depth` feature.

```{r}
install.packages("sandwich")
library(lmtest)
library(sandwich)
lm_fit = lm(y ~ X - 1)

robust_vcov = vcovHC(lm_fit, type = "HC0")  # HC0 = classic White estimator
robust_test = coeftest(lm_fit, vcov. = robust_vcov)
pval_robust_depth = robust_test["Xdepth", "Pr(>|t|)"]
pval_robust_depth
```

Now assume the errors are mean-centered and homoskedastic. 

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 where j is the index of the `depth` feature.

```{r}
lm_fit = lm(y ~ X - 1)
summary_fit = summary(lm_fit)
pval_depth = coef(summary_fit)["Xdepth", "Pr(>|t|)"]
pval_depth
```

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 and beta_k = 0 where j is the index of the `depth` feature and k is the index of the `table` feature.

```{r}
install.packages("car")
library(car)
joint_test = linearHypothesis(lm_fit, c("Xdepth = 0", "Xtable = 0"))
print(joint_test)
```

Now assume the errors are mean-centered and heteroskedastic. This is the scenario where you employ the Huber-White estimator.

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 where j is the index of the `depth` feature.

```{r}
robust_vcov = vcovHC(lm_fit, type = "HC0")  # HC0 = classic Huber-White
robust_test = coeftest(lm_fit, vcov. = robust_vcov)
pval_robust_depth = robust_test["Xdepth", "Pr(>|t|)"]
pval_robust_depth
```

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 and beta_k = 0 where j is the index of the `depth` feature and k is the index of the `table` feature.

```{r}
robust_vcov = vcovHC(lm_fit, type = "HC0")
joint_test = linearHypothesis(lm_fit,c("Xdepth = 0","Xtable = 0"),vcov. = robust_vcov, test = "Chisq")  
joint_test
```

# Problem 4a: Inference for Bernoulli Response Models

We load up the Glass dataset below. The goal is to predict and understand the effects of features on whether or not the glass is of type 1.

```{r}
install.packages("mlbench")
library(mlbench)
rm(list = ls())
data(Glass)
glass = na.omit(Glass)
glass$Type = ifelse(glass$Type == 1, 1, 0)
```

Fit a probit regression using all features and report p-values for H_0: beta_j = 0 for all features. Using the `glm` function with `family = binomial(link = "probit")`.

```{r}
probit_fit = glm(Type ~ ., data = glass, family = binomial(link = "probit"))
summary(probit_fit)
```

Run the omnibus test at level alpha=5% to see if any of these features are useful in predicting the probability of Type=1.

```{r}
null_model = glm(Type ~ 1, data = glass, family = binomial(link = "probit"))
full_model = glm(Type ~ ., data = glass, family = binomial(link = "probit"))
LR_stat = 2 * (logLik(full_model)[1] - logLik(null_model)[1])
df = length(coef(full_model)) - length(coef(null_model))
pval = 1 - pchisq(LR_stat, df)
LR_stat
pval
pval < 0.05  # TRUE = reject H0 → at least one feature is useful
```


Predict the probability of glass being of type 1 if the sample had average amounts of all features.

```{r}
x_vec_avg = data.frame(t(apply(glass, 2, mean)))
# Remove the response variable (Type) from the average vector
x_vec_avg = x_vec_avg[ , setdiff(names(x_vec_avg), "Type")]
probit_pred = predict(probit_fit, newdata = x_vec_avg, type = "response")
probit_pred
```

Add quadratic terms to all the features and fit a new model. Check if these additional features are justified at level alpha=5%.

```{r}
predictor_names = setdiff(names(glass), "Type")
glass_quad = glass
for (var in predictor_names) {
  glass_quad[[paste0(var, "_sq")]] = glass[[var]]^2
}

probit_linear = glm(Type ~ ., data = glass, family = binomial(link = "probit"))

probit_quad = glm(Type ~ ., data = glass_quad, family = binomial(link = "probit"))
LR_stat = 2 * (logLik(probit_quad)[1] - logLik(probit_linear)[1])
df = length(coef(probit_quad)) - length(coef(probit_linear))
pval = 1 - pchisq(LR_stat, df)
LR_stat
pval
pval < 0.05  # TRUE = reject H0 → quadratic terms improve the model
```

# Problem 4b: Inference for Poisson Count Response Model

We load up the insurance dataset below. The goal is to predict and understand the effects of features on number of car insurance claims (the `Claims` column).

```{r}
rm(list = ls())
insur = MASS::Insurance
insur$Group = factor(insur$Group, ordered = FALSE)
insur$Age = factor(insur$Age, ordered = FALSE)
```

Fit a poisson count model (AKA "Poisson regression") to the data and report p-values for H_0: beta_j = 0 for all features. Using the `glm` function with `family="poisson"` defaults to the log link.

```{r}
poisson_fit = glm(Claims ~ District + Group + Age, data = insur, family = "poisson")
summary(poisson_fit)
```

Predict the number of claims (to the nearest claim) for a someone who lives in a major city, who's age 26, has a 1.8L engine car and has only one policy.

```{r}
#TO-DO
```

Now fit a Poisson count model that includes the interaction of Age and Holders. Test whether the addition of these interactions is warranted at level alpha=5%.

```{r}
#TO-DO
```

# Problem 4c: Inference for Negative Binomial Count Response Model

Fit a Negative Binomial count model (AKA "negative binomial regression") to the data and report p-values for H_0: beta_j = 0 for all features. To do this use the `glm.nb` which defaults to the log link.

```{r}
#TO-DO
```

Predict the number of claims (to the nearest claim) for a someone who lives in a major city, who's age 26, has a 1.8L engine car and has only one policy.

```{r}
new = data.frame(
  District = factor("1", levels = levels(insur$District)),
  Group = factor("1.5-2l", levels = levels(insur$Group)),
  Age = factor("25-29", levels = levels(insur$Age)),
  Holders = 1
)

poisson_fit = glm(Claims ~ District + Group + Age + offset(log(Holders)),data = insur, family = "poisson")
predicted_claims = predict(poisson_fit, newdata = new, type = "response")
round(predicted_claims)
```

Now fit a Negative Binomial count model that includes the interaction of Age and Holders. Test whether the addition of these interactions is warranted at level alpha=5%.

```{r}
library(MASS)
#w/out interaction
nb_base = glm.nb(Claims ~ District + Group + Age + Holders, data = insur)

# extended NB model with interaction between Age and Holders
nb_interact = glm.nb(Claims ~ District + Group + Age * Holders, data = insur)

#Lr test (test if interaction improves fit)
LR_stat = 2 * (logLik(nb_interact)[1] - logLik(nb_base)[1])

#df= number of additional parameters (one for each extra interaction term)
df = attr(logLik(nb_interact), "df") - attr(logLik(nb_base), "df")
pval = 1 - pchisq(LR_stat, df)
LR_stat
df
pval
pval < 0.05  # TRUE = interaction is warranted
```


Were there any substantive differences between the inference of prediction you found between the Poisson and Negative Binomial models?

No, similar pt predicitions expected


# Problem 4d: Inference for the Weibull Survival Model

Let's load up data from a trial of usrodeoxycholic acid.

```{r}
rm(list = ls())
library(survival)
udca2 = na.omit(survival::udca2)
?udca2
survival_time = udca2$futime
uncensored_dummy = udca2$status
udca2$id = NULL
udca2$status = NULL
udca2$futime = NULL
udca2$endpoint = NULL
```

We now create a surv object and print out the first 20 entries.

```{r}
surv_obj = Surv(survival_time, uncensored_dummy)
rm(survival_time, uncensored_dummy)
head(surv_obj, 20)
```

What do the "+" signs mean in the above print out?


censored observations

Fit a Weibull regression model to all features and report p-values for H_0: beta_j = 0 for all features.

```{r}
weibull_model = survreg(surv_obj ~ ., data = udca2, dist = "weibull")
summary(weibull_model)
pvals = summary(weibull_model)$table[, "p"]
pvals
```

Predict the survival time for a subject with the UDCA treatment (i.e. trt = 1), stage = 1, bili = 1.5 and riskscore = 4.0.
```{r}
new_patient = data.frame(
  trt = 1,
  stage = 1,
  bili = 1.5,
  riskscore = 4.0
)

predicted_time = predict(weibull_model, newdata = new_patient, type = "response")
predicted_time
```

Run the omnibus test at alpha=5%.

```{r}
null_model = survreg(surv_obj ~ 1, data = udca2, dist = "weibull")
full_model = survreg(surv_obj ~ ., data = udca2, dist = "weibull")
LR_stat = 2 * (logLik(full_model)[1] - logLik(null_model)[1])
df = length(coef(full_model)) - 1  # minus 1 to exclude intercept
pval = 1 - pchisq(LR_stat, df)
LR_stat
pval
pval < 0.05 
```

Run the test to see if the variables stage, bili and riskscore are important in predicting survival at alpha=5%.

```{r}
full_model = survreg(surv_obj ~ ., data = udca2, dist = "weibull")
red_model = survreg(surv_obj ~ . - stage - bili - riskscore, data = udca2, dist = "weibull")
LR_stat = 2 * (logLik(full_model)[1] - logLik(red_model)[1])
df = 3 # = number removed
pval = 1 - pchisq(LR_stat, df)
LR_stat
pval
pval < 0.05
```
